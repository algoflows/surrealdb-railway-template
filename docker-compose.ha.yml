version: '3.8'

# SurrealDB Minimal HA - Coordinator High Availability
#
# 5 services: 3 PD (HA) + 1 TiKV + 1 SurrealDB
# Cost: ~$20-30/month
# Protects against coordinator failures (most common)

services:
  # PD Cluster - 3 nodes for consensus (HA)
  pd0:
    image: pingcap/pd:v8.5.0
    environment:
      - RAILWAY_STATIC_URL
    command:
      - --name=pd0
      - --client-urls=http://0.0.0.0:2379
      - --peer-urls=http://0.0.0.0:2380
      - --advertise-client-urls=http://pd0:2379
      - --advertise-peer-urls=http://pd0:2380
      - --initial-cluster=pd0=http://pd0:2380,pd1=http://pd1:2380,pd2=http://pd2:2380
      - --data-dir=/data/pd0
      - --log-level=${PD_LOG_LEVEL:-info}
      - --log-format=json
      - --lease=3
      - --enable-grpc-gateway=true
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.25'
        reservations:
          memory: 128M
          cpus: '0.1'
    volumes:
      - pd0_data:/data
    restart: unless-stopped
    healthcheck:
      test: /pd-ctl health | jq -e ".[] | select(.name == \"$(hostname)\").health"
      start_period: 10s
      retries: 5
      timeout: 10s
      interval: 30s

  pd1:
    image: pingcap/pd:v8.5.0
    environment:
      - RAILWAY_STATIC_URL
    command:
      - --name=pd1
      - --client-urls=http://0.0.0.0:2379
      - --peer-urls=http://0.0.0.0:2380
      - --advertise-client-urls=http://pd1:2379
      - --advertise-peer-urls=http://pd1:2380
      - --initial-cluster=pd0=http://pd0:2380,pd1=http://pd1:2380,pd2=http://pd2:2380
      - --data-dir=/data/pd1
      - --log-level=${PD_LOG_LEVEL:-info}
      - --log-format=json
      - --lease=3
      - --enable-grpc-gateway=true
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.25'
        reservations:
          memory: 128M
          cpus: '0.1'
    volumes:
      - pd1_data:/data
    restart: unless-stopped
    healthcheck:
      test: /pd-ctl health | jq -e ".[] | select(.name == \"$(hostname)\").health"
      start_period: 10s
      retries: 5
      timeout: 10s
      interval: 30s

  pd2:
    image: pingcap/pd:v8.5.0
    environment:
      - RAILWAY_STATIC_URL
    command:
      - --name=pd2
      - --client-urls=http://0.0.0.0:2379
      - --peer-urls=http://0.0.0.0:2380
      - --advertise-client-urls=http://pd2:2379
      - --advertise-peer-urls=http://pd2:2380
      - --initial-cluster=pd0=http://pd0:2380,pd1=http://pd1:2380,pd2=http://pd2:2380
      - --data-dir=/data/pd2
      - --log-level=${PD_LOG_LEVEL:-info}
      - --log-format=json
      - --lease=3
      - --enable-grpc-gateway=true
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.25'
        reservations:
          memory: 128M
          cpus: '0.1'
    volumes:
      - pd2_data:/data
    restart: unless-stopped
    healthcheck:
      test: /pd-ctl health | jq -e ".[] | select(.name == \"$(hostname)\").health"
      start_period: 10s
      retries: 5
      timeout: 10s
      interval: 30s

  # Single TiKV node (still single point, but coordinator is HA)
  tikv0:
    image: pingcap/tikv:v8.5.0
    environment:
      - RAILWAY_STATIC_URL
    command:
      - --addr=0.0.0.0:20160
      - --advertise-addr=tikv0:20160
      - --status-addr=0.0.0.0:20180
      - --data-dir=/data/tikv0
      - --pd=pd0:2379,pd1:2379,pd2:2379
      - --log-level=${TIKV_LOG_LEVEL:-info}
      - --log-format=json
      - --config-check-interval=10s
      - --grpc-concurrency=2
      - --grpc-concurrent-stream=512
    deploy:
      resources:
        limits:
          memory: 1G # Slightly bigger since it's the only storage node
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.5'
    volumes:
      - tikv0_data:/data
    depends_on:
      pd0:
        condition: service_healthy
      pd1:
        condition: service_healthy
      pd2:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: /tikv-ctl --host $(hostname):20160 metrics
      start_period: 15s
      retries: 5
      timeout: 10s
      interval: 30s

  # SurrealDB query engine (scale replicas via Railway dashboard)
  surrealdb:
    image: surrealdb/surrealdb:latest
    environment:
      - PORT=8000
      - SURREAL_USER=${SURREAL_USER:-root}
      - SURREAL_PASS=${SURREAL_PASS:-root}
      - RAILWAY_STATIC_URL
    ports:
      - '${PORT:-8000}:8000'
    command:
      - start
      - --log=${SURREAL_LOG_LEVEL:-info}
      - --user=${SURREAL_USER:-root}
      - --pass=${SURREAL_PASS:-root}
      - --bind=0.0.0.0:8000
      - --query-timeout=30s
      - --transaction-timeout=10s
      - tikv://pd0:2379,pd1:2379,pd2:2379
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.5'
    depends_on:
      tikv0:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ['CMD-SHELL', 'curl -f http://localhost:8000/health || exit 1']
      start_period: 20s
      retries: 5
      timeout: 10s
      interval: 30s

volumes:
  pd0_data:
  pd1_data:
  pd2_data:
  tikv0_data:
# HA Strategy:
# 1. PD Cluster (3 nodes) - Can lose 1 node and stay operational
# 2. TiKV Single Node - Still a single point, but coordinator failures are more common
# 3. SurrealDB - Scale replicas via Railway dashboard for query layer HA
#
# Next Step: When you need storage HA, add tikv1 and tikv2 nodes
